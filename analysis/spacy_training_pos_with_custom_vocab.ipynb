{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to combine Vocab with Vectors for POS Tagger training\n",
    "\n",
    "This notebook shows that training POS Tagger on combined vocab + vectors is not more effective than training it on just the vectors.\n",
    "While training POS Tagger without custom vocab was added to DVC, this is just a demonstration and therefore will not be part of any pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to do this first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && dvc pull && dvc repro vocab-model.dvc && dvc repro init-model-vectors-300.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy  # make sure it is our own master, not the latest spact from pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kk385830/spacy-pl-utils/analysis\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Polish = spacy.util.get_lang_class('pl')\n",
    "vocab = spacy.vocab.Vocab().from_disk('../training/vocab/vocab/')\n",
    "vectors = spacy.vectors.Vectors().from_disk('../training/vectors-300/vocab/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect differences between vocab lexemes and vector keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146089"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143478, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4918752717281726814, 4347571224981593329, 14399288562721897527, 1591739283372538942}\n",
      "CPU times: user 40 ms, sys: 0 ns, total: 40 ms\n",
      "Wall time: 37.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "missing_keys =  set([key for key in vectors.keys() if not key in vocab])\n",
    "print(missing_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2615\n",
      "['A.E.', 'T. T.', 'R. A.', 'x.', 'R.U.', 'Z.Z.', 'O. Ź.', 'O. W.', 'K. L.', 'W.Ś.']\n",
      "CPU times: user 136 ms, sys: 0 ns, total: 136 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vk = set(vectors.keys())\n",
    "missing_keys_2 = set([key.text for key in vocab if not key.orth in vk])\n",
    "print(len(missing_keys_2))\n",
    "print(list(missing_keys_2)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding vectors to their corresponding lexemes\n",
    "Note that `missing_keys` contain list of keys from the vectors that cannot be added to any of the lexemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 840 ms, sys: 1min 22s, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for idx, (key, vector) in enumerate(vectors.items()):\n",
    "    try:\n",
    "        vocab.set_vector(key, vector)\n",
    "    except Exception:\n",
    "        assert(key in missing_keys)  # if this fails, we have calculated missing keys badly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146089"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors_length  # should be 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternative approach\n",
    "Spacy cli `train` command uses `util.load_model` which does not match vocab to the vectors by itself (at least not in our case).\n",
    "\n",
    "Possible reasons for this behavior:\n",
    "- either vocab or vectors are not well-formatted and the module is confused\n",
    "- my matching strategy implemented above is wrong - vocab and vectors represent completely different sets of words even though their ids/hashes match (at least in most cases)\n",
    "- vocab cannot contain additional information that is present and that is the reason for failed matches\n",
    "- using `util.load_model` to specify vocab for vectors is broken and we should report it as an issue\n",
    "\n",
    "Need to further inspect why it is the case, see:\n",
    "https://github.com/explosion/spaCy/blob/021d04069a9c49bc29c6f4415e3d2b4b7a02012e/spacy/cli/train.py#L93\n",
    "\n",
    "(as well as the code below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = spacy.util.load_model('../training/vectors-300/', vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289760"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.vocab.length  # nearly 2x of what it should be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that vocab is duplicated, as if vectors were generated for a separate vocab than provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.vocab.vectors_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the vocab and train polish POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Polish(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline  # should be empty so that we can add the pos tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = spacy.pipeline.Tagger(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add tag map to tagger !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin training the tagger\n",
    "We cannot use train from spacy cli for loading our vocab correctly, code below is a copied version of it using our loaded language, containing vocab as well as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.gold import GoldCorpus\n",
    "from tqdm import tqdm\n",
    "\n",
    "import plac\n",
    "from pathlib import Path\n",
    "from thinc.neural._classes.model import Model\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from spacy.attrs import PROB, IS_OOV, CLUSTER, LANG\n",
    "from spacy.gold import GoldCorpus, minibatch\n",
    "from spacy.util import prints\n",
    "from spacy import util\n",
    "from spacy import about\n",
    "from spacy import displacy\n",
    "from spacy.compat import json_dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/training/pos-train.json'\n",
    "n_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang='pl'\n",
    "output_dir='../training/pos-tagger-poc-2/'\n",
    "train_data='../data/training/pos-train.json'\n",
    "dev_data='../data/training/pos-validation.json'\n",
    "n_iter=10\n",
    "n_sents=None\n",
    "use_gpu=3\n",
    "# vectors=,  # THIS IS PROVIDED MANUALLY (VECTORS DEFINED ABOVE)\n",
    "no_tagger=False\n",
    "no_parser=True\n",
    "no_entities=True\n",
    "gold_preproc=False\n",
    "meta_path=None\n",
    "verbose=False  # DO NOT USE THIS\n",
    "version='0.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _render_parses(i, to_render):\n",
    "    to_render[0].user_data['title'] = \"Batch %d\" % i\n",
    "    with Path('/tmp/entities.html').open('w') as file_:\n",
    "        html = displacy.render(to_render[:5], style='ent', page=True)\n",
    "        file_.write(html)\n",
    "    with Path('/tmp/parses.html').open('w') as file_:\n",
    "        html = displacy.render(to_render[:5], style='dep', page=True)\n",
    "        file_.write(html)\n",
    "\n",
    "\n",
    "def print_progress(itn, losses, dev_scores, cpu_wps=0.0, gpu_wps=0.0):\n",
    "    scores = {}\n",
    "    for col in ['dep_loss', 'tag_loss', 'uas', 'tags_acc', 'token_acc',\n",
    "                'ents_p', 'ents_r', 'ents_f', 'cpu_wps', 'gpu_wps']:\n",
    "        scores[col] = 0.0\n",
    "    scores['dep_loss'] = losses.get('parser', 0.0)\n",
    "    scores['ner_loss'] = losses.get('ner', 0.0)\n",
    "    scores['tag_loss'] = losses.get('tagger', 0.0)\n",
    "    scores.update(dev_scores)\n",
    "    scores['cpu_wps'] = cpu_wps\n",
    "    scores['gpu_wps'] = gpu_wps or 0.0\n",
    "    tpl = ''.join((\n",
    "        '{:<6d}',\n",
    "        '{dep_loss:<10.3f}',\n",
    "        '{ner_loss:<10.3f}',\n",
    "        '{uas:<8.3f}',\n",
    "        '{ents_p:<8.3f}',\n",
    "        '{ents_r:<8.3f}',\n",
    "        '{ents_f:<8.3f}',\n",
    "        '{tags_acc:<8.3f}',\n",
    "        '{token_acc:<9.3f}',\n",
    "        '{cpu_wps:<9.1f}',\n",
    "        '{gpu_wps:.1f}',\n",
    "    ))\n",
    "    print(tpl.format(itn, **scores))\n",
    "\n",
    "\n",
    "def print_results(scorer):\n",
    "    results = {\n",
    "        'TOK': '%.2f' % scorer.token_acc,\n",
    "        'POS': '%.2f' % scorer.tags_acc,\n",
    "        'UAS': '%.2f' % scorer.uas,\n",
    "        'LAS': '%.2f' % scorer.las,\n",
    "        'NER P': '%.2f' % scorer.ents_p,\n",
    "        'NER R': '%.2f' % scorer.ents_r,\n",
    "        'NER F': '%.2f' % scorer.ents_f}\n",
    "    util.print_table(results, title=\"Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_from = 0.2 by default\n",
      "dropout_to = 0.2 by default\n",
      "dropout_decay = 0.0 by default\n",
      "batch_from = 1 by default\n",
      "batch_to = 16 by default\n",
      "batch_compound = 1.001 by default\n",
      "max_doc_len = 5000 by default\n",
      "learn_rate = 0.001 by default\n",
      "optimizer_B1 = 0.9 by default\n",
      "optimizer_B2 = 0.999 by default\n",
      "optimizer_eps = 1e-08 by default\n",
      "L2_penalty = 1e-06 by default\n",
      "grad_norm_clip = 1.0 by default\n",
      "Itn.  Dep Loss  NER Loss  UAS     NER P.  NER R.  NER F.  Tag %   Token %  CPU WPS  GPU WPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/537931 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.000     0.000     0.000   0.000   0.000   0.000   85.020  100.000  13458.4  7556.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/537931 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     0.000     0.000     0.000   0.000   0.000   0.000   86.609  100.000  11628.4  11374.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/537931 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2     0.000     0.000     0.000   0.000   0.000   0.000   87.542  100.000  1143.4   1978.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/537931 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3     0.000     0.000     0.000   0.000   0.000   0.000   88.115  100.000  1311.3   2081.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/537931 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4     0.000     0.000     0.000   0.000   0.000   0.000   88.544  100.000  11394.7  3088.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/537931 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5     0.000     0.000     0.000   0.000   0.000   0.000   88.774  100.000  3778.6   4093.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/537931 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6     0.000     0.000     0.000   0.000   0.000   0.000   88.873  100.000  11337.2  3001.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/537931 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7     0.000     0.000     0.000   0.000   0.000   0.000   88.662  100.000  12124.9  3955.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/537931 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8     0.000     0.000     0.000   0.000   0.000   0.000   28.393  100.000  13094.9  11887.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9     0.000     0.000     0.000   0.000   0.000   0.000   25.100  100.000  10901.4  9552.1\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "# copied from spacy.train\n",
    "util.fix_random_seed()\n",
    "util.set_env_log(True)\n",
    "n_sents = None\n",
    "output_path = util.ensure_path(output_dir)\n",
    "train_path = util.ensure_path(train_data)\n",
    "dev_path = util.ensure_path(dev_data)\n",
    "meta_path = util.ensure_path(meta_path)\n",
    "if not output_path.exists():\n",
    "    output_path.mkdir()\n",
    "if not train_path.exists():\n",
    "    prints(train_path, title=Messages.M050, exits=1)\n",
    "if dev_path and not dev_path.exists():\n",
    "    prints(dev_path, title=Messages.M051, exits=1)\n",
    "if meta_path is not None and not meta_path.exists():\n",
    "    prints(meta_path, title=Messages.M020, exits=1)\n",
    "meta = util.read_json(meta_path) if meta_path else {}\n",
    "if not isinstance(meta, dict):\n",
    "    prints(Messages.M053.format(meta_type=type(meta)),\n",
    "           title=Messages.M052, exits=1)\n",
    "meta.setdefault('lang', lang)\n",
    "meta.setdefault('name', 'unnamed')\n",
    "\n",
    "pipeline = ['tagger', 'parser', 'ner']\n",
    "if no_tagger and 'tagger' in pipeline:\n",
    "    pipeline.remove('tagger')\n",
    "if no_parser and 'parser' in pipeline:\n",
    "    pipeline.remove('parser')\n",
    "if no_entities and 'ner' in pipeline:\n",
    "    pipeline.remove('ner')\n",
    "\n",
    "# Take dropout and batch size as generators of values -- dropout\n",
    "# starts high and decays sharply, to force the optimizer to explore.\n",
    "# Batch size starts at 1 and grows, so that we make updates quickly\n",
    "# at the beginning of training.\n",
    "dropout_rates = util.decaying(util.env_opt('dropout_from', 0.2),\n",
    "                              util.env_opt('dropout_to', 0.2),\n",
    "                              util.env_opt('dropout_decay', 0.0))\n",
    "batch_sizes = util.compounding(util.env_opt('batch_from', 1),\n",
    "                               util.env_opt('batch_to', 16),\n",
    "                               util.env_opt('batch_compound', 1.001))\n",
    "max_doc_len = util.env_opt('max_doc_len', 5000)\n",
    "corpus = GoldCorpus(train_path, dev_path, limit=n_sents)\n",
    "n_train_words = corpus.count_train()\n",
    "\n",
    "# lang_class = util.get_lang_class(lang)\n",
    "# nlp = lang_class()\n",
    "meta['pipeline'] = pipeline\n",
    "nlp.meta.update(meta)\n",
    "# if vectors:\n",
    "#     print(\"Load vectors model\", vectors)\n",
    "#     util.load_model(vectors, vocab=nlp.vocab)\n",
    "#     for lex in nlp.vocab:\n",
    "#         values = {}\n",
    "#         for attr, func in nlp.vocab.lex_attr_getters.items():\n",
    "#             # These attrs are expected to be set by data. Others should\n",
    "#             # be set by calling the language functions.\n",
    "#             if attr not in (CLUSTER, PROB, IS_OOV, LANG):\n",
    "#                 values[lex.vocab.strings[attr]] = func(lex.orth_)\n",
    "#         lex.set_attrs(**values)\n",
    "#         lex.is_oov = False\n",
    "# for name in pipeline:\n",
    "#     nlp.add_pipe(nlp.create_pipe(name), name=name)\n",
    "# if parser_multitasks:\n",
    "#     for objective in parser_multitasks.split(','):\n",
    "#         nlp.parser.add_multitask_objective(objective)\n",
    "# if entity_multitasks:\n",
    "#     for objective in entity_multitasks.split(','):\n",
    "#         nlp.entity.add_multitask_objective(objective)\n",
    "\n",
    "\n",
    "optimizer = nlp.begin_training(lambda: corpus.train_tuples, device=use_gpu)\n",
    "nlp._optimizer = None\n",
    "\n",
    "print(\"Itn.  Dep Loss  NER Loss  UAS     NER P.  NER R.  NER F.  Tag %   Token %  CPU WPS  GPU WPS\")\n",
    "try:\n",
    "    train_docs = corpus.train_docs(nlp, projectivize=True, noise_level=0.0,\n",
    "                                   gold_preproc=gold_preproc, max_length=0)\n",
    "    train_docs = list(train_docs)\n",
    "    for i in range(n_iter):\n",
    "        with tqdm(total=n_train_words, leave=False) as pbar:\n",
    "            losses = {}\n",
    "            for batch in minibatch(train_docs, size=batch_sizes):\n",
    "                batch = [(d, g) for (d, g) in batch if len(d) < max_doc_len]\n",
    "                if not batch:\n",
    "                    continue\n",
    "                docs, golds = zip(*batch)\n",
    "                nlp.update(docs, golds, sgd=optimizer,\n",
    "                           drop=next(dropout_rates), losses=losses)\n",
    "                pbar.update(sum(len(doc) for doc in docs))\n",
    "\n",
    "        with nlp.use_params(optimizer.averages):\n",
    "            util.set_env_log(False)\n",
    "            epoch_model_path = output_path / ('model%d' % i)\n",
    "            nlp.to_disk(epoch_model_path)\n",
    "            nlp_loaded = util.load_model_from_path(epoch_model_path)\n",
    "            dev_docs = list(corpus.dev_docs(\n",
    "                            nlp_loaded,\n",
    "                            gold_preproc=gold_preproc))\n",
    "            nwords = sum(len(doc_gold[0]) for doc_gold in dev_docs)\n",
    "            start_time = timer()\n",
    "            scorer = nlp_loaded.evaluate(dev_docs, verbose)\n",
    "            end_time = timer()\n",
    "            if use_gpu < 0:\n",
    "                gpu_wps = None\n",
    "                cpu_wps = nwords/(end_time-start_time)\n",
    "            else:\n",
    "                gpu_wps = nwords/(end_time-start_time)\n",
    "                with Model.use_device('cpu'):\n",
    "                    nlp_loaded = util.load_model_from_path(epoch_model_path)\n",
    "                    dev_docs = list(corpus.dev_docs(\n",
    "                                    nlp_loaded, gold_preproc=gold_preproc))\n",
    "                    start_time = timer()\n",
    "                    scorer = nlp_loaded.evaluate(dev_docs)\n",
    "                    end_time = timer()\n",
    "                    cpu_wps = nwords/(end_time-start_time)\n",
    "            acc_loc = (output_path / ('model%d' % i) / 'accuracy.json')\n",
    "            with acc_loc.open('w') as file_:\n",
    "                file_.write(json_dumps(scorer.scores))\n",
    "            meta_loc = output_path / ('model%d' % i) / 'meta.json'\n",
    "            meta['accuracy'] = scorer.scores\n",
    "            meta['speed'] = {'nwords': nwords, 'cpu': cpu_wps,\n",
    "                             'gpu': gpu_wps}\n",
    "            meta['vectors'] = {'width': nlp.vocab.vectors_length,\n",
    "                               'vectors': len(nlp.vocab.vectors),\n",
    "                               'keys': nlp.vocab.vectors.n_keys}\n",
    "            meta['lang'] = nlp.lang\n",
    "            meta['pipeline'] = pipeline\n",
    "            meta['spacy_version'] = '>=%s' % about.__version__\n",
    "            meta.setdefault('name', 'model%d' % i)\n",
    "            meta.setdefault('version', version)\n",
    "\n",
    "            with meta_loc.open('w') as file_:\n",
    "                file_.write(json_dumps(meta))\n",
    "            util.set_env_log(True)\n",
    "        print_progress(i, losses, scorer.scores, cpu_wps=cpu_wps,\n",
    "                       gpu_wps=gpu_wps)\n",
    "finally:\n",
    "    print(\"Saving model...\")\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        final_model_path = output_path / 'model-final'\n",
    "        nlp.to_disk(final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the looks of it, adding vocab to the model does not improve performance in any way (overfitting after iteration #6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
